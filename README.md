ðŸ“Œ Key Design Notes

Component	        Notes
backend/app/	    Use FastAPI routers, keep logic clean and modular
llm_server/	        Keep your LLaMA model files and serving logic separate for GPU optimization
knowledge_base/	    Plug in FAISS, ChromaDB, or other RAG tools (with embeddings from SentenceTransformers or HuggingFace)
frontend/	        Use Vite or CRA, talk to backend using Axios or Fetch
environments/	    Helps you export + recreate everything on your airgapped IBM server
scripts/	        Youâ€™ll need startup + deployment scripts, especially offline


zeni /
â”œâ”€â”€ backend/                     # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                 # All API routes
â”‚   â”‚   â”œâ”€â”€ core/                # Config, constants, utils
â”‚   â”‚   â”œâ”€â”€ services/            # Logic to talk to LLaMA, KB, etc.
â”‚   â”‚   â”œâ”€â”€ models/              # Pydantic models / schemas
â”‚   â”‚   â”œâ”€â”€ tasks/               # Background / async tasks
â”‚   â”‚   â””â”€â”€ main.py              # Entry point for FastAPI app
â”‚   â””â”€â”€ tests/                   # Unit/integration tests
â”‚
â”œâ”€â”€ llm_server/                  # Local LLaMA 370B integration
â”‚   â”œâ”€â”€ launcher/                # Script to load/serve LLaMA (e.g. llama-cpp-python or HuggingFace)
â”‚   â”œâ”€â”€ quantized_models/        # LLaMA model files
â”‚   â””â”€â”€ config/                  # Model config, tokenizer, etc.
â”‚
â”œâ”€â”€ knowledge_base/             # RAG components
â”‚   â”œâ”€â”€ data/                    # Uploaded & indexed documents
â”‚   â”œâ”€â”€ indexing/                # Code to parse and embed documents
â”‚   â””â”€â”€ retriever/               # Vector store and search logic
â”‚
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ services/            # API calls to backend
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ environments/              # Dependency management
â”‚   â”œâ”€â”€ environment.yml         # Conda + pip packages for backend
â”‚   â””â”€â”€ frontend.env            # Node version, npm/yarn setup
â”‚
â”œâ”€â”€ scripts/                   # One-off scripts or utilities
â”‚   â”œâ”€â”€ start_server.sh         # Start backend + LLM + KB
â”‚   â”œâ”€â”€ sync_to_airgapped.py    # Create export bundles
â”‚   â””â”€â”€ validate_env.py
â”‚
â””â”€â”€ README.md

Sample curl 


curl --location 'http://localhost:8001/    generate' \--header 'Content-Type: application/json' \--data '{  "context": [    {"role": "user", "content": "Rewrite         professionally glad to meet you         today mark, hope to connect in         future to discuss further on the         project "}  ]}'


Future work 

        +------------------------+
        |   Frontend UI (Chat)  |
        +----------+------------+
                   |
                   â†“
        +------------------------+
        |   App Backend Router   |
        +----------+-------------+
         |                        |
         â†“                        â†“
+-------------------+    +---------------------+
| Sync: HTTP to LLM |    | Async: Push to Queue|
+-------------------+    +---------------------+
                                 â†“
                      +------------------------+
                      |  Worker Calls LLM API  |
                      +------------------------+
